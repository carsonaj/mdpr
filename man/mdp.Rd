% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp.R
\name{mdp}
\alias{mdp}
\title{Policy minimizer for Markov decision processes via value iteration}
\usage{
mdp(states, action_fn, prob_fn, cost_fn, final_cost_fn, initial_state, horizon)
}
\arguments{
\item{states}{- vector of states}

\item{action_fn}{- function with input: current_state - state and output: possible_actions - vector of actions}

\item{prob_fn}{- function with input: (current_state - state, action_taken - action, next_state - state) and output: probability - double in 0 to 1}

\item{cost_fn}{- function with input: (time - positive integer less than horizon, current_state - state, action_taken - action, next_state - state) and output: cost}

\item{final_cost_fn}{- function with input: current_state - state and output: final_cost - scalar}

\item{initial_state}{- state}

\item{horizon}{- positive integer}
}
\value{
policy_fn - function, input: (time - positive integer less than horizon, current_state - state) and output: action_taken - action
}
\description{
Policy minimizer for Markov decision processes via value iteration
}
