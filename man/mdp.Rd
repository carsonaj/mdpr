% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mdp.R, R/mdp_wrapper.R
\name{mdp}
\alias{mdp}
\title{Policy minimizer for Markov decision processes via value iteration}
\usage{
mdp(states, action_fn, prob_fn, cost_fn, horizon)

mdp(states, action_fn, prob_fn, cost_fn, horizon)
}
\arguments{
\item{states}{- vector of states}

\item{action_fn}{- function, input: state,}

\item{prob_fn}{- function, input: current state, action taken, next state,}

\item{cost_fn}{- function, input: time, current state, action taken, next state,}

\item{horizon}{- positive integer}

\item{final_cost_fn}{- function with input: current_state - state and output: final_cost - scalar}

\item{initial_state}{- state}
}
\value{
policy_fn - function, input: (time - positive integer less than horizon, current_state - state) and output: action_taken - action

policy_fn - function, input: time, current state,
}
\description{
Policy minimizer for Markov decision processes via value iteration

Policy minimizer for Markov decision processes
}
